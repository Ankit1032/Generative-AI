{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f7a2bb6",
   "metadata": {},
   "source": [
    "### Dynamic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59f438cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langchain.agents import create_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
    "from langchain.agents.structured_output import ToolStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b13054ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Loaded Succesfully!\n"
     ]
    }
   ],
   "source": [
    "# Load .env variables\n",
    "load_dotenv()\n",
    "\n",
    "# Read API key\n",
    "gpt_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Set the environment variable that OpenAI expects\n",
    "if gpt_api_key:\n",
    "    print(\"API Loaded Succesfully!\")\n",
    "else:\n",
    "    raise ValueError(\"GPT_API not found in .env file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "01056130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Use Pydantic instead of dataclass for better compatibility\n",
    "class ResponseFormat(BaseModel):\n",
    "    \"\"\"Response schema for the agent.\"\"\"\n",
    "    llm_model_name_used: str = Field(description=\"The name of the LLM model used\")\n",
    "    answer_to_user_query: str = Field(description=\"The answer to the user's question\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "48e642f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "basic_model = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "advanced_model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "@wrap_model_call\n",
    "def dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:\n",
    "    \"\"\"Choose model based on conversation complexity.\"\"\"\n",
    "    message_count = len(request.state[\"messages\"])\n",
    "\n",
    "    if message_count > 10:\n",
    "        # Use an advanced model for longer conversations\n",
    "        model = advanced_model\n",
    "    else:\n",
    "        model = basic_model\n",
    "\n",
    "    ##below is my made code to store the LLM model name so we can see it in output\n",
    "    request.state[\"selected_model_name\"] = getattr(model, \"model\", getattr(model, \"name\", None)) or str(model)\n",
    "\n",
    "    return handler(request.override(model=model))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe7a49c",
   "metadata": {},
   "source": [
    "#### What is message_count?\n",
    "\n",
    "##### This counts how many back-and-forth messages have been exchanged in the conversation, not words.\n",
    "\n",
    "##### Conversation 1: Single message\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the color of the sun? One word answer only\"}\n",
    "]\n",
    "##### message_count = 1 (only 1 message)\n",
    "\n",
    "------------------------------------------------------------------------------------------------------\n",
    "\n",
    "##### Conversation 2: Multiple messages\n",
    "\n",
    "messages = [\n",
    "\n",
    "    {\"role\": \"user\", \"content\": \"Hi\"},\n",
    "\n",
    "    {\"role\": \"assistant\", \"content\": \"Hello! How can I help?\"},\n",
    "\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about Python\"},\n",
    "\n",
    "    {\"role\": \"assistant\", \"content\": \"Python is a programming language...\"},\n",
    "\n",
    "    {\"role\": \"user\", \"content\": \"What about machine learning?\"},\n",
    "\n",
    "]\n",
    "\n",
    "##### message_count = 5 (5 messages total)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc3eff5",
   "metadata": {},
   "source": [
    "#### The idea:\n",
    "\n",
    "1. Short conversations (≤10 messages) → Use cheaper, faster model (gpt-4o-mini)\n",
    "2. Long conversations (>10 messages) → Use smarter, more expensive model (gpt-4o) because complex discussions might need better reasoning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcd63cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=basic_model,  # Default model\n",
    "    middleware=[dynamic_model_selection],\n",
    "    system_prompt = \"Answer with only one word. No more than one word\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "77e3578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What is the color is sun. One word answer only\"}]},\n",
    "    response_format=ToolStrategy(ResponseFormat)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d3f71069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Model Used: gpt-4o-mini-2024-07-18\n",
      "Answer: White.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Extract the information\n",
    "final_message = response['messages'][-1]\n",
    "model_used = final_message.response_metadata.get('model_name', 'Unknown')\n",
    "answer = final_message.content\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Model Used: {model_used}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cacabc",
   "metadata": {},
   "source": [
    "##### Invoking the bigger model with 10+ messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "de94086f",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_messages = [\n",
    "    {\"role\": \"user\", \"content\": \"What is the color of Sun\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the color of Space\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the weather like?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is Machine\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me about Python.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is a programming language.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What about JavaScript?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is Marvel\"},\n",
    "    {\"role\": \"user\", \"content\": \"How about databases?\"},\n",
    "    {\"role\": \"user\", \"content\": \"What store data.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the color of the Moon?\"}  # 11th message\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b4e0238",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.invoke({\"messages\": conversation_messages}, response_format=ToolStrategy(ResponseFormat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "059594a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Model Used: gpt-4o-2024-08-06\n",
      "Answer: The color of the Moon as seen from Earth can vary based on several factors, including its position in the sky, atmospheric conditions, and the time of observation. Generally, the Moon appears:\n",
      "\n",
      "1. **Gray or White**: When high in the sky, the Moon usually appears to be a grayish-white color due to the reflection of sunlight off its surface, which is composed of various minerals.\n",
      "\n",
      "2. **Yellow or Orange**: When the Moon is low on the horizon, such as during a moonrise or moonset, it can appear yellow, orange, or even reddish. This is due to Rayleigh scattering, the same phenomenon that causes sunsets to be red or orange. The Earth's atmosphere scatters shorter blue wavelengths of light more than the longer red wavelengths, causing the Moon to take on a warmer hue.\n",
      "\n",
      "3. **Reddish**: During a total lunar eclipse, the Moon can appear reddish or copper-colored, often referred to as a \"blood moon.\" This is due to sunlight being filtered and refracted by the Earth's atmosphere, which scatters the blue light and allows the red wavelengths to illuminate the Moon.\n",
      "\n",
      "These variations make the Moon an intriguing subject for observation and photography.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Extract the information\n",
    "final_message = response['messages'][-1]\n",
    "model_used = final_message.response_metadata.get('model_name', 'Unknown')\n",
    "answer = final_message.content\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(f\"Model Used: {model_used}\")\n",
    "print(f\"Answer: {answer}\")\n",
    "print(f\"{'='*50}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

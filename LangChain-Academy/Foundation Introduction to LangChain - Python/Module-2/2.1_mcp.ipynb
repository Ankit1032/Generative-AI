{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b91a58f8",
   "metadata": {},
   "source": [
    "MCP is defined by Anthropic, the group who made it as an open protocol that standardizes how your LLM applications connect to and work with your tools and data sources.\n",
    "\n",
    "Simple Analogy : Take example of USB - Type C wire : I can connect my phone to my laptop just by plugging the USB to my laptop and Type C end to my phone. If we didnt have this wire, we would probably download some custom softwares to connect our devices and transport data between the 2.\n",
    "\n",
    "Creating tools and providing context to different model providers used to look like below\n",
    "\n",
    "    AI APP 1  -->  Prompt  -->  Tools  -->  Resources\n",
    "    AI APP 2  -->  Prompt  -->  Tools  -->  Resources\n",
    "    AI APP 3  -->  Prompt  -->  Tools  -->  Resources\n",
    "    AI APP 4  -->  Prompt  -->  Tools  -->  Resources\n",
    "\n",
    "It was a never ending web of API calls and database to connect to your agent for every application you tried to build\n",
    "\n",
    "Anthropic solves this problem firsthand, which is why they suggested a universal model context protocol for model providers and tool builders to use.\n",
    "\n",
    "In this standard, the MCP Host(Our Agent) hosts an MCP client, which communicates to the MCP server. In our case, we can think of the MCP host as the AI application or the agent\n",
    "\n",
    "These MCP servers can expose tools, resources, like read only data or prompt templates to the client.\n",
    "\n",
    "Once we have built an MCP server with our tools and context, it is very easy to share with other projects or developers streamlining future agent builds.\n",
    "\n",
    "There is a huge open source community of MCP Servers that other people have built, which we can easily share insert into our agent and other types of AI applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "717edf63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db64c0fd-9355-49e4-8290-2f2ae08eeef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import asyncio\n",
    "\n",
    "# Fix for Windows issues in Jupyter notebooks\n",
    "if sys.platform == \"win32\":\n",
    "    # 1. Use ProactorEventLoop for subprocess support\n",
    "    if not isinstance(asyncio.get_event_loop_policy(), asyncio.WindowsProactorEventLoopPolicy):\n",
    "        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n",
    "    \n",
    "    # 2. Redirect stderr to avoid fileno() error when launching MCP servers\n",
    "    if \"ipykernel\" in sys.modules:\n",
    "        sys.stderr = sys.__stderr__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d701224",
   "metadata": {},
   "source": [
    "## Local MCP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "749bbcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-mcp-adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f11678d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"local_server\": {\n",
    "                \"transport\": \"stdio\",\n",
    "                \"command\": \"python\",\n",
    "                \"args\": [\"resources/2.1_mcp_server.py\"],\n",
    "            }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "184db1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tools\n",
    "tools = await client.get_tools()\n",
    "\n",
    "# get resources\n",
    "resources = await client.get_resources(\"local_server\")\n",
    "\n",
    "# get prompts\n",
    "prompt = await client.get_prompt(\"local_server\", \"prompt\")\n",
    "prompt = prompt[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f2b65fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructuredTool(name='search_web', description='Search the web for information', args_schema={'properties': {'query': {'title': 'Query', 'type': 'string'}}, 'required': ['query'], 'title': 'search_webArguments', 'type': 'object'}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x000001B565FD9580>)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29a8f61f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Blob 1878611928768]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85dce5e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    You are a helpful assistant that answers user questions about LangChain, LangGraph and LangSmith.\\n\\n    You can use the following tools/resources to answer user questions:\\n    - search_web: Search the web for information\\n    - github_file: Access the langchain-ai repo files\\n\\n    If the user asks a question that is not related to LangChain, LangGraph or LangSmith, you should say \"I\\'m sorry, I can only answer questions about LangChain, LangGraph and LangSmith.\"\\n\\n    You may try multiple tool and resource calls to answer the user\\'s question.\\n\\n    You may also ask clarifying questions to the user to better understand their question.\\n    '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d548fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    tools=tools,\n",
    "    system_prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5256ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me about the langchain-mcp-adapters library\")]},\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3efb5bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Tell me about the langchain-mcp-adapters library', additional_kwargs={}, response_metadata={}, id='fee86c4f-9d1d-4e96-b615-2d8bc64f9a0c'),\n",
      "              AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_TkixeHg9GNmSyzsykNji3DRr', 'function': {'arguments': '{\"query\":\"langchain-mcp-adapters library\"}', 'name': 'search_web'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 188, 'total_tokens': 208, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_f0bc439dc3', 'id': 'chatcmpl-Cwk73XEpHiG8Vzo24QOP4bFHvD0mk', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--4a3ff6a5-c173-4826-924b-a48edafb3144-0', tool_calls=[{'name': 'search_web', 'args': {'query': 'langchain-mcp-adapters library'}, 'id': 'call_TkixeHg9GNmSyzsykNji3DRr', 'type': 'tool_call'}], usage_metadata={'input_tokens': 188, 'output_tokens': 20, 'total_tokens': 208, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"query\": \"langchain-mcp-adapters library\",\\n  \"follow_up_questions\": null,\\n  \"answer\": null,\\n  \"images\": [],\\n  \"results\": [\\n    {\\n      \"url\": \"https://pypi.org/project/langchain-mcp-adapters/\",\\n      \"title\": \"langchain-mcp-adapters - PyPI\",\\n      \"content\": \"This library provides a lightweight wrapper that makes Anthropic Model Context Protocol (MCP) tools compatible with LangChain and LangGraph.\",\\n      \"score\": 0.99999285,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph\",\\n      \"title\": \"MCP Adapters for LangChain and LangGraph\",\\n      \"content\": \"# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.\",\\n      \"score\": 0.9999875,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://github.com/langchain-ai/langchainjs-mcp-adapters\",\\n      \"title\": \"langchain-ai/langchainjs-mcp-adapters: ** THIS REPO ... - GitHub\",\\n      \"content\": \"This library provides a lightweight wrapper to allow Model Context Protocol (MCP) services to be used with LangChain.js.\",\\n      \"score\": 0.9999831,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://reference.langchain.com/python/langchain_mcp_adapters/\",\\n      \"title\": \"langchain-mcp-adapters\",\\n      \"content\": \"# `langchain-mcp-adapters`¶. Client for connecting to multiple MCP servers and loading LC tools/resources. This module provides the `MultiServerMCPClient` class for managing connections to multiple MCP servers and loading tools, prompts, and resources from them. Loads LangChain-compatible tools, prompts and resources from MCP servers. | \\\\\\\\_\\\\\\\\_init\\\\\\\\_\\\\\\\\_ (`langchain_mcp_adapters.client.MultiServerMCPClient.__init__`)\\\\\">\\\\\\\\_\\\\\\\\_init\\\\\\\\_\\\\\\\\_ | Initialize a `MultiServerMCPClient` with MCP servers connections. | session  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.session`)\\\\\">session | Connect to an MCP server and initialize a session. | get\\\\\\\\_tools  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.get_tools`)\\\\\">get\\\\\\\\_tools | Get a list of all tools from all connected servers. | get\\\\\\\\_resources  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.get_resources`)\\\\\">get\\\\\\\\_resources | Get resources from MCP server(s). **TYPE:** `dict[str,`  Connection  `module-attribute`  (`langchain_mcp_adapters.sessions.Connection`)\\\\\">Connection] | None   **DEFAULT:** `None` |. ### load\\\\\\\\_mcp\\\\\\\\_tools `async` ¶. (langchain_mcp_adapters.callbacks.Callbacks)\\\\\">Callbacks | None = None,  tool_interceptors: list[            ToolCallInterceptor (langchain_mcp_adapters.interceptors.ToolCallInterceptor)\\\\\">ToolCallInterceptor] | None = None,  server_name: str | None = None,  tool_name_prefix: bool = False, ) -> list[            BaseTool (langchain_core.tools.BaseTool)\\\\\">BaseTool]. **TYPE:**  Connection  `module-attribute`  (`langchain_mcp_adapters.sessions.Connection`)\\\\\">Connection | None   **DEFAULT:** `None` |. ### load\\\\\\\\_mcp\\\\\\\\_resources `async` ¶. | \\\\\\\\_\\\\\\\\_call\\\\\\\\_\\\\\\\\_  `async`  (`langchain_mcp_adapters.interceptors.ToolCallInterceptor.__call__`)\\\\\">\\\\\\\\_\\\\\\\\_call\\\\\\\\_\\\\\\\\_ | Intercept tool execution with control over handler invocation.\",\\n      \"score\": 0.9999813,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://www.npmjs.com/package/@langchain/mcp-adapters\",\\n      \"title\": \"langchain/mcp-adapters - NPM\",\\n      \"content\": \"import{createAgent} from \\\\\"langchain\\\\\"; import{ChatOpenAI} from\\\\\"@langchain/openai\\\\\"; import{MultiServerMCPClient} from\\\\\"@langchain/mcp-adapters\\\\\";// Create client and connect to server const client = new MultiServerMCPClient({// Global tool configuration options// Whether to throw on errors if a tool fails to load (optional, default: true) throwOnLoadError true,// Whether to prefix tool names with the server name (optional, default: false) prefixToolNameWithServerName false,// Optional additional prefix for tool names (optional, default: \\\\\"\\\\\") additionalToolNamePrefix \\\\\"\\\\\",// Use standardized content block format in tool outputs useStandardContentBlocks true,// Behavior when a server fails to connect: \\\\\"throw\\\\\" (default) or \\\\\"ignore\\\\\" onConnectionError \\\\\"ignore\\\\\",// Server configuration mcpServers{// adds a STDIO connection to a server named \\\\\"math\\\\\" math{transport \\\\\"stdio\\\\\", command \\\\\"npx\\\\\", args[\\\\\"-y\\\\\",\\\\\"@modelcontextprotocol/server-math\\\\\"],// Restart configuration for stdio transport restart{enabled true, maxAttempts 3, delayMs 1000,},},// here\\'s a filesystem server filesystem{transport \\\\\"stdio\\\\\", command \\\\\"npx\\\\\", args[\\\\\"-y\\\\\",\\\\\"@modelcontextprotocol/server-filesystem\\\\\"],},// Sreamable HTTP transport example, with auth headers and automatic SSE fallback disabled (defaults to enabled) weather{url\\\\\"https://example.com/weather/mcp\\\\\", headers{Authorization \\\\\"Bearer token123\\\\\",} automaticSSEFallback false},// OAuth 2.0 authentication (recommended for secure servers)\\\\\"oauth-protected-server\\\\\"{url\\\\\"https://protected.example.com/mcp\\\\\", authProvider new MyOAuthProvider({// Your OAuth provider implementation redirectUrl\\\\\"https://myapp.com/oauth/callback\\\\\", clientMetadata{redirect_uris[\\\\\"https://myapp.com/oauth/callback\\\\\"], client_name \\\\\"My MCP Client\\\\\", scope\\\\\"mcp:read mcp:write\\\\\"}}),// Can still include custom headers for non-auth purposes headers{\\\\\"User-Agent\\\\\"\\\\\"My-MCP-Client/1.0\\\\\"}},// how to force SSE, for old servers that are known to only support SSE (streamable HTTP falls back automatically if unsure) github{transport \\\\\"sse\\\\\",// also works with \\\\\"type\\\\\" field instead of \\\\\"transport\\\\\" url\\\\\"https://example.com/mcp\\\\\", reconnect{enabled true, maxAttempts 5, delayMs 2000,},},},}); const tools = await client.\",\\n      \"score\": 0.99995565,\\n      \"raw_content\": null\\n    }\\n  ],\\n  \"response_time\": 0.46,\\n  \"request_id\": \"4e7b7896-5b47-4b96-94d6-d9c39d108688\"\\n}', 'id': 'lc_d61ad301-eac3-4191-9378-43750417822e'}], name='search_web', id='42744b49-98ee-407c-9841-4ce76e1cca57', tool_call_id='call_TkixeHg9GNmSyzsykNji3DRr', artifact={'structured_content': {'result': {'query': 'langchain-mcp-adapters library', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://pypi.org/project/langchain-mcp-adapters/', 'title': 'langchain-mcp-adapters - PyPI', 'content': 'This library provides a lightweight wrapper that makes Anthropic Model Context Protocol (MCP) tools compatible with LangChain and LangGraph.', 'score': 0.99999285, 'raw_content': None}, {'url': 'https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph', 'title': 'MCP Adapters for LangChain and LangGraph', 'content': '# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.', 'score': 0.9999875, 'raw_content': None}, {'url': 'https://github.com/langchain-ai/langchainjs-mcp-adapters', 'title': 'langchain-ai/langchainjs-mcp-adapters: ** THIS REPO ... - GitHub', 'content': 'This library provides a lightweight wrapper to allow Model Context Protocol (MCP) services to be used with LangChain.js.', 'score': 0.9999831, 'raw_content': None}, {'url': 'https://reference.langchain.com/python/langchain_mcp_adapters/', 'title': 'langchain-mcp-adapters', 'content': '# `langchain-mcp-adapters`¶. Client for connecting to multiple MCP servers and loading LC tools/resources. This module provides the `MultiServerMCPClient` class for managing connections to multiple MCP servers and loading tools, prompts, and resources from them. Loads LangChain-compatible tools, prompts and resources from MCP servers. | \\\\_\\\\_init\\\\_\\\\_ (`langchain_mcp_adapters.client.MultiServerMCPClient.__init__`)\">\\\\_\\\\_init\\\\_\\\\_ | Initialize a `MultiServerMCPClient` with MCP servers connections. | session  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.session`)\">session | Connect to an MCP server and initialize a session. | get\\\\_tools  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.get_tools`)\">get\\\\_tools | Get a list of all tools from all connected servers. | get\\\\_resources  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.get_resources`)\">get\\\\_resources | Get resources from MCP server(s). **TYPE:** `dict[str,`  Connection  `module-attribute`  (`langchain_mcp_adapters.sessions.Connection`)\">Connection] | None   **DEFAULT:** `None` |. ### load\\\\_mcp\\\\_tools `async` ¶. (langchain_mcp_adapters.callbacks.Callbacks)\">Callbacks | None = None,  tool_interceptors: list[            ToolCallInterceptor (langchain_mcp_adapters.interceptors.ToolCallInterceptor)\">ToolCallInterceptor] | None = None,  server_name: str | None = None,  tool_name_prefix: bool = False, ) -> list[            BaseTool (langchain_core.tools.BaseTool)\">BaseTool]. **TYPE:**  Connection  `module-attribute`  (`langchain_mcp_adapters.sessions.Connection`)\">Connection | None   **DEFAULT:** `None` |. ### load\\\\_mcp\\\\_resources `async` ¶. | \\\\_\\\\_call\\\\_\\\\_  `async`  (`langchain_mcp_adapters.interceptors.ToolCallInterceptor.__call__`)\">\\\\_\\\\_call\\\\_\\\\_ | Intercept tool execution with control over handler invocation.', 'score': 0.9999813, 'raw_content': None}, {'url': 'https://www.npmjs.com/package/@langchain/mcp-adapters', 'title': 'langchain/mcp-adapters - NPM', 'content': 'import{createAgent} from \"langchain\"; import{ChatOpenAI} from\"@langchain/openai\"; import{MultiServerMCPClient} from\"@langchain/mcp-adapters\";// Create client and connect to server const client = new MultiServerMCPClient({// Global tool configuration options// Whether to throw on errors if a tool fails to load (optional, default: true) throwOnLoadError true,// Whether to prefix tool names with the server name (optional, default: false) prefixToolNameWithServerName false,// Optional additional prefix for tool names (optional, default: \"\") additionalToolNamePrefix \"\",// Use standardized content block format in tool outputs useStandardContentBlocks true,// Behavior when a server fails to connect: \"throw\" (default) or \"ignore\" onConnectionError \"ignore\",// Server configuration mcpServers{// adds a STDIO connection to a server named \"math\" math{transport \"stdio\", command \"npx\", args[\"-y\",\"@modelcontextprotocol/server-math\"],// Restart configuration for stdio transport restart{enabled true, maxAttempts 3, delayMs 1000,},},// here\\'s a filesystem server filesystem{transport \"stdio\", command \"npx\", args[\"-y\",\"@modelcontextprotocol/server-filesystem\"],},// Sreamable HTTP transport example, with auth headers and automatic SSE fallback disabled (defaults to enabled) weather{url\"https://example.com/weather/mcp\", headers{Authorization \"Bearer token123\",} automaticSSEFallback false},// OAuth 2.0 authentication (recommended for secure servers)\"oauth-protected-server\"{url\"https://protected.example.com/mcp\", authProvider new MyOAuthProvider({// Your OAuth provider implementation redirectUrl\"https://myapp.com/oauth/callback\", clientMetadata{redirect_uris[\"https://myapp.com/oauth/callback\"], client_name \"My MCP Client\", scope\"mcp:read mcp:write\"}}),// Can still include custom headers for non-auth purposes headers{\"User-Agent\"\"My-MCP-Client/1.0\"}},// how to force SSE, for old servers that are known to only support SSE (streamable HTTP falls back automatically if unsure) github{transport \"sse\",// also works with \"type\" field instead of \"transport\" url\"https://example.com/mcp\", reconnect{enabled true, maxAttempts 5, delayMs 2000,},},},}); const tools = await client.', 'score': 0.99995565, 'raw_content': None}], 'response_time': 0.46, 'request_id': '4e7b7896-5b47-4b96-94d6-d9c39d108688'}}}),\n",
      "              AIMessage(content='The `langchain-mcp-adapters` library is a lightweight wrapper that allows integration of Anthropic Model Context Protocol (MCP) tools with LangChain and LangGraph. It facilitates converting MCP tools into LangChain- and LangGraph-compatible tools, enabling interaction with multiple MCP servers simultaneously. This makes it easier to incorporate a growing ecosystem of MCP tool servers into LangChain and LangGraph applications, allowing agents to access tools across multiple MCP servers and leverage existing tool servers efficiently.\\n\\nFor more details, you can visit the [PyPI page](https://pypi.org/project/langchain-mcp-adapters/) or the [MCP Adapters changelog](https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph).', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 154, 'prompt_tokens': 1886, 'total_tokens': 2040, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_f0bc439dc3', 'id': 'chatcmpl-Cwk79sMHMIMp9cP1SV1jD0iTRU3LJ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--8da00eba-4cc1-4485-874a-0a62eaabb5b0-0', usage_metadata={'input_tokens': 1886, 'output_tokens': 154, 'total_tokens': 2040, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847409a3",
   "metadata": {},
   "source": [
    "## Online MCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc183a0",
   "metadata": {},
   "source": [
    "Open MCP Servers\n",
    "1. https://mcp.so/servers\n",
    "2. https://github.com/punkpeye/awesome-mcp-servers\n",
    "3. https://glama.ai/mcp/servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b2895fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"time\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": \"uvx\",\n",
    "            \"args\": [\n",
    "                \"mcp-server-time\",\n",
    "                \"--local-timezone=America/New_York\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e264dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=\"gpt-4.1-nano\",\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4725cee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What time is it?', additional_kwargs={}, response_metadata={}, id='42d32ed4-0553-4084-b55f-ab19f39c2eb8'),\n",
      "              AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_KS0DkUrJkpw0S1p9jBApU0if', 'function': {'arguments': '{\"timezone\":\"America/New_York\"}', 'name': 'get_current_time'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 215, 'total_tokens': 233, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_7f8eb7d1f9', 'id': 'chatcmpl-CwkBQb8haLSUkE6JaqqiVmxtJuDiY', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--2d921b31-1f79-41f5-b867-53916c807b07-0', tool_calls=[{'name': 'get_current_time', 'args': {'timezone': 'America/New_York'}, 'id': 'call_KS0DkUrJkpw0S1p9jBApU0if', 'type': 'tool_call'}], usage_metadata={'input_tokens': 215, 'output_tokens': 18, 'total_tokens': 233, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"timezone\": \"America/New_York\",\\n  \"datetime\": \"2026-01-11T02:29:46-05:00\",\\n  \"day_of_week\": \"Sunday\",\\n  \"is_dst\": false\\n}', 'id': 'lc_b2b21953-345a-4731-a67e-a86441bd157e'}], name='get_current_time', id='2d1bb7cf-10c2-424c-b017-ce3eb219271e', tool_call_id='call_KS0DkUrJkpw0S1p9jBApU0if'),\n",
      "              AIMessage(content='The current time in New York is 2:29 AM on Sunday, January 11, 2026.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 24, 'prompt_tokens': 292, 'total_tokens': 316, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4.1-nano-2025-04-14', 'system_fingerprint': 'fp_7f8eb7d1f9', 'id': 'chatcmpl-CwkBTytu4BBzEp8i1gepHDeBtoi5G', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--bda36048-ac67-4cb3-828d-17c953212062-0', usage_metadata={'input_tokens': 292, 'output_tokens': 24, 'total_tokens': 316, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "question = HumanMessage(content=\"What time is it?\")\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [question]}\n",
    ")\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bc5152",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
